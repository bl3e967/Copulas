{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n", "<hr>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Copula estimation using Kernel Desnity Estimation\n", "\n", "\n", "\n", "\n", "**Objective:** \n", "Here, we will investigate whether using Kernel Density Estimation seems effective for modelling a copula using empirical data, rather than using a parametric model. "]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["from statsmodels.distributions.empirical_distribution import ECDF\n", "from copulas.bivariate.base import Bivariate\n", "from copulas.multivariate import GaussianMultivariate\n", "\n", "class Parameter():\n", "    def __init__(self,\n", "                resolution,\n", "                start_year, \n", "                start_month=1, \n", "                start_day=1,\n", "                n_years=0,\n", "                n_months=0, \n", "                n_days=0): \n", "        \n", "        self.Resolution = resolution\n", "        \n", "        end_year = start_year + n_years\n", "        end_month = start_month + n_months\n", "        end_day = start_day + n_days \n", "        self.Start = datetime(start_year,start_month,start_day,9,30,0)\n", "        self.End = datetime(end_year, end_month, end_day,16,30,0)\n", "    \n", "class Data():\n", "    def __init__(self, df):\n", "        self._df = df.dropna()\n", "    \n", "    @property \n", "    def prices(self):\n", "        return self._df\n", "    \n", "    @property\n", "    def returns(self):\n", "        return self._df.diff() \n", "    \n", "    @property\n", "    def columns(self):\n", "        return self._df.columns\n", "    \n", "    def plot(self, *args, **kwargs): \n", "        return self._df.plot(*args, **kwargs)\n", "    \n", "class EmpiricalCopula():\n", "    def __init__(self, X):\n", "        '''\n", "            args:\n", "                X: numpy array of shape (n_pairs, 2)\n", "        '''\n", "        self.data = X\n", "        self._len = X.shape[0]\n", "    \n", "    def __len__(self):\n", "        return self._len\n", "    \n", "    @staticmethod \n", "    def _get_bin_width(self, data):\n", "        return freedman_diaconis(data)\n", "    \n", "    def cdf(self, u, v):\n", "        '''returns C(u,v) = P(u <= U, v <= V)'''\n", "        cnt = np.sum((self.data[:,0] <= u) & (self.data[:,1] <= v))\n", "        return float(cnt / self._len)\n", "    \n", "    def pdf(self, u, v):\n", "        pass \n", "    \n", "    def cond_pdf(u, given_p):\n", "        '''P(U<=u | V=v) = dC(u,v)/dv = P(U<=u, V=v)/p(V=v)'''\n", "        pass \n", "        \n", "def emp_copula_test():\n", "    u = np.expand_dims(np.arange(0,100,1)+1, axis=1)\n", "    v = np.expand_dims(np.arange(0,100,1)+1, axis=1)\n", "    x = np.concatenate((u,v),axis=1)\n", "    print(\"u:\",u.shape)\n", "    print(\"v:\",v.shape)\n", "    print(\"x:\",x.shape)\n", "        \n", "    test_copula = EmpiricalCopula(x)\n", "    assert len(test_copula) == len(u), f\"{len(test_copula)} != {len(u)}\"\n", "    \n", "    pvalue = test_copula.cdf(50,50)\n", "    assert pvalue == 0.5, f\"p = {pvalue} not 0.5\"\n", "    pvalue = test_copula.cdf(25,25)\n", "    assert pvalue == 0.25, f\"p = {pvalue} not 0.25\"\n", "    pvalue = test_copula.cdf(100,0)\n", "    assert pvalue == 0.0, f\"p={pvalue} not 0\"\n", "    pvalue = test_copula.cdf(0,100)\n", "    assert pvalue == 0.0, f\"p={pvalue} not 0\"\n", "    pvalue = test_copula.cdf(20,100)\n", "    assert pvalue == 0.2, f\"p={pvalue} not 0.2\"\n", "    pvalue = test_copula.cdf(100,20)\n", "    assert pvalue == 0.2, f\"p={pvalue} not 0.2\"\n", "    pvalue = test_copula.cdf(100,100)\n", "    assert pvalue == 1.0, f\"p={pvalue} not 1.0\"\n", "    \n", "emp_copula_test()"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import random\n", "\n", "def sample_from_bivariate(x_domain, y_domain, weights, n_samples):\n", "    x_domain = x_domain.ravel()\n", "    y_domain = y_domain.ravel()\n", "    weights = np.nan_to_num(weights.ravel()) # account for nans\n", "    \n", "    population = np.array([x_domain, y_domain]).T\n", "    print(population)\n", "    return np.array(random.choices(population, weights, k=n_samples))\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# Utils\n", "def np_remove_nan(x): \n", "    return x[np.logical_not(np.isnan(x).any(axis=1))]\n", "\n", "def np_remove_inf(x):\n", "    return x[np.logical_not(np.isinf(x).any(axis=1))]\n", "\n", "def np_remove_inf_1D(x):\n", "    return x[~np.isinf(x)]\n", "\n", "def np_remove_nan_1D(x):\n", "    return x[~np.isinf(x)]\n", "\n", "# test remove nan \n", "data = np.array([[np.inf,np.inf],\n", "                 [1,2],\n", "                 [1,3],\n", "                 [np.nan, 4],\n", "                 [5,6]])\n", "\n", "data_no_nan = np_remove_nan(data)\n", "data_no_inf = np_remove_inf(data)\n", "data_no_non_num = np_remove_nan(np_remove_inf(data))\n", "\n", "check_for_nan = lambda x: True not in np.isnan(x)\n", "check_for_inf = lambda x: True not in np.isinf(x)\n", "\n", "assert check_for_nan(data_no_nan)\n", "assert check_for_inf(data_no_inf)\n", "assert check_for_nan(data_no_non_num) and check_for_inf(data_no_non_num)\n", "assert data_no_non_num.shape == (3,2)\n", "\n", "# print(\"-\"*15)\n", "# print(\"input\")\n", "# print(data)\n", "# print(\"-\"*15)\n", "# print(data_no_nan)\n", "# print(\"-\"*15)\n", "# print(data_no_inf)\n", "# print(\"-\"*15)\n", "# print(data_no_non_num)"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["## model comparison criteria\n", "\n", "def AIC(log_lik, n_params):\n", "    '''\n", "        AIC = 2k - 2*ln(L)\n", "        Args: \n", "            log_lik: total log likelihood over all sampoles\n", "            n_samples: number of samples\n", "            n_params: number of parameters\n", "    '''\n", "    return 2 * n_params - 2 * log_lik\n", "\n", "def BIC(log_lik, n_samples, n_params):\n", "    '''\n", "        BIC = k*ln(N) - 2*ln(L)\n", "        where \n", "            k = number of parameters\n", "            N = number of samples \n", "            L = likelihood\n", "        Args: \n", "            log_lik: total log likelihood over all sampoles\n", "            n_samples: number of samples\n", "            n_params: number of parameters\n", "    '''\n", "    return k * np.log(n_samples) - 2 * log_lik\n", "\n", "# test AICimport, BIC\n", "_n_samples = 1000\n", "_dist1_params = {\"loc\":0, \"scale\":1}\n", "_dist2_params = {\"loc\":10, \"scale\":1}\n", "\n", "from scipy.stats import norm\n", "samples_dist1 = norm.rvs(**_dist1_params, size=_n_samples)\n", "samples_dist2 = norm.rvs(**_dist2_params, size=_n_samples)\n", "\n", "loglik_samp1_on_dist1 = np.sum(norm.logpdf(samples_dist1, **_dist1_params))\n", "loglik_samp2_on_dist1 = np.sum(norm.logpdf(samples_dist2, **_dist1_params))\n", "loglik_samp2_on_dist2 = np.sum(norm.logpdf(samples_dist2, **_dist2_params))\n", "loglik_samp1_on_dist2 = np.sum(norm.logpdf(samples_dist1, **_dist2_params))\n", "\n", "aic_samp1_on_dist1 = AIC(loglik_samp1_on_dist1, len(_dist1_params))\n", "aic_samp2_on_dist1 = AIC(loglik_samp2_on_dist1, len(_dist1_params))\n", "aic_samp2_on_dist2 = AIC(loglik_samp2_on_dist2, len(_dist2_params))\n", "aic_samp1_on_dist2 = AIC(loglik_samp1_on_dist2, len(_dist2_params))\n", "\n", "_msg = \"AIC values have unexpected magnitudes\"\n", "assert aic_samp1_on_dist1 < aic_samp2_on_dist1, _msg\n", "assert aic_samp2_on_dist2 < aic_samp2_on_dist1, _msg\n", "\n", "__plot__ = False \n", "if __plot__: \n", "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n", "    fig.suptitle('Normal samples')\n", "    ax1.hist(samples_dist1, bins=100)\n", "    ax2.hist(samples_dist2, bins=100)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# QuantBook Analysis Tool \n", "# For more information see [https://www.quantconnect.com/docs/research/overview]\n", "qb = QuantBook()\n", "\n", "# parameters\n", "START_YEAR = 2005\n", "START_MONTH = 1\n", "START_DAY = 1\n", "N_YEARS = 5\n", "N_MONTHS = 0\n", "N_DAYS = 0\n", "RESOLUTION = Resolution.Daily\n", "PARAM = Parameter(RESOLUTION, START_YEAR, START_MONTH, START_DAY,\n", "                  N_YEARS, N_MONTHS, N_DAYS)\n", "\n", "# Specify list of correlated tickers for S&P 500 \n", "tickers = [\"SPY\",\"XLK\", \"VGT\", \"IYW\", \"IGV\"]\n", "\n", "# register tickers to quantbook\n", "for ticker in tickers: \n", "    qb.AddEquity(ticker)\n", "    \n", "# get historical prices\n", "history = qb.History(tickers, PARAM.Start, PARAM.End, PARAM.Resolution)\n", "\n", "# Unpack dataframe\n", "Open = history['open'].unstack(level=0)\n", "Close = history['close'].unstack(level=0)\n", "High = history['high'].unstack(level=0)\n", "Low = history['low'].unstack(level=0)\n", "\n", "# create data object\n", "close = Data(Close)\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# Empirically estimate marginal distributions\n", "marginal_dist = {}\n", "for asset in close.columns:\n", "    marginal_dist[asset] = ECDF(close.returns[asset])\n", "\n", "marginal_values = pd.DataFrame()\n", "for asset in close.columns: \n", "    marginal_values[asset] = close.returns[asset].apply(marginal_dist[asset])"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Calculate Kendall Tau\n", "corr = close.returns.corr(method='kendall')\n", "corr.style.background_gradient(cmap='viridis')"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["def get_max_corr_pair(corr_matrix): \n", "    sol = (corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n", "                 .stack()\n", "                 .sort_values(ascending=False))\n", "    \n", "    col1 = close.columns[sol.index[sol == max(sol)].labels[0]].values[0]\n", "    col2 = close.columns[sol.index[sol == max(sol)].labels[1]].values[0]\n", "    return(col1, col2)\n", "\n", "pair_override = None #('IYW RUTTRZ1RC7L1','XLK RGRPZX100F39')\n", "pair = get_max_corr_pair(corr)\n", "# pair = pair_override if not None else pair \n", "print(\"Pair with maximum correlation: {}\".format(pair))\n", "\n", "data = np.array(marginal_values[list(pair)].values)\n", "close.prices[list(pair)].plot(figsize=(20,10))\n", "close.returns[list(pair)].plot(figsize=(20,10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Copula Estimate using Transformation Method\n", "\n", "Let $\\Phi$ be the standard normal cdf and $\\phi$ its density. Then the random vector $(X,Y) = (\\Phi^{-1}(U), \\Phi^{-1}(V))$ has normally distributed margins and is supported on the full $\\mathbb{R}^2$. By Sklar's Theroem, its density $f$ can be written as: \n", "\n", "$$f(x,y) = c(\\Phi(x), \\Phi(y))\\phi(x)\\phi(y)$$\n", "\n", "To estimate this density, we transform samples $(U_i,V_i)$ to $(X_i,Y_i) = (\\Phi^{-1}(U_i), \\Phi^{-1}(V_i))$ for $i = 1,...,n$. To this transformed sample, we now apply the standard kernel density estimator for $(x,y) \\in \\mathbb{R}^2$: \n", "\n", "$$\\hat{f}_n(x,y) = \\frac{1}{n}\\sum_{i=1}^{n}K_{b_n}(x-X_i)K_{b_n}(y-Y_i)$$\n", "\n", "Hence, the copula density can be expressed as : \n", "\n", "$$\n", "    \\hat{c}_n(u,v) = \\frac{\\sum_{i=1}^{n}K_{b_n}\\big(\\Phi^{-1}(u) - \\Phi^{-1}(U_i)\\big)K_{b_n}\\big(\\Phi^{-1}(v) - \\Phi^{-1}(V_i)\\big)}{n\\phi(\\Phi^{-1}(u))\\phi(\\Phi^{-1}(v))}\n", "$$\n", "for all $(u,v) \\in [0,1]^2$"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["#-----------------------------------------------------------------------------------------\n", "# Fit KDE estiamte on transformed data\n", "#-----------------------------------------------------------------------------------------\n", "from time import time\n", "from scipy.stats import norm\n", "import statsmodels.api as sm\n", "\n", "# transform samples to R^2 domain\n", "X_ = norm.ppf(data)\n", "X_ = np_remove_inf(X_)\n", "\n", "# fit a kernel density estimate \n", "_BANDWIDTH = 'cv_ls' #'normal_reference' # 'cv_ml', 'cv_ls'\n", "kernel = sm.nonparametric.KDEMultivariate\n", "t = time()\n", "model = kernel(data=X_, var_type='cc', bw=_BANDWIDTH)\n", "elapsed = time() - t \n", "\n", "print(\"Model fit in {} seconds\".format(elapsed))\n", "print(\"Model bandwidth {}\".format(model.bw))"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["#-------------------------------------------------------------------------------------------------\n", "# generate pdf in R^2 domain\n", "#-------------------------------------------------------------------------------------------------\n", "from time import time \n", "\n", "_width = 0.01\n", "_axis_offset = 1\n", "x_valuesR = np.arange(np.floor(np.min(X_)), np.ceil(np.max(X_)),_width)\n", "y_valuesR = np.arange(np.floor(np.min(X_)-_axis_offset), np.ceil(np.max(X_)+_axis_offset),_width)\n", "\n", "x_meshR, y_meshR = np.meshgrid(x_valuesR, y_valuesR)\n", "data_predictR = pd.DataFrame({\"U\":x_meshR.ravel(), \"V\":y_meshR.ravel()})\n", "\n", "t = time()\n", "z_valuesR = model.pdf(data_predict=data_predictR)\n", "elapsed = time() - t\n", "print(\"{} seconds for {} data\".format(elapsed, len(data_predictR)))"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["#--------------------------------------------------------------------------------------\n", "# Plot the transformed distribution and KDE estimate in R^2 domain\n", "#--------------------------------------------------------------------------------------\n", "z_meshR = z_valuesR.reshape(x_meshR.shape)\n", "\n", "# contour plot of resulting kde estimate of copula pdf in [0,1]^2\n", "plt.figure(figsize=(12.5,10))\n", "plt.contourf(x_meshR, y_meshR, z_meshR, 100, cmap='viridis')\n", "plt.colorbar()\n", "plt.title(\"Contour plot of copula pdf using Kernel Density Estimation in $\\mathbb{R}^2$ domain\")\n", "\n", "# get samples from distribution in R^2 and plot \n", "n_samples=len(X_)\n", "samples = sample_from_bivariate(x_meshR, y_meshR, z_meshR, n_samples)\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(samples[:,0], samples[:,1], s=0.3)\n", "plt.title(\"Scatter Plot of samples from Kernel Density Estimate\")\n", "\n", "# plot dataset in R^2 domain\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(X_[:,0], X_[:,1], s=0.3)\n", "plt.title(\"Scatter Plot of Dataset\")"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["#----------------------------------------------------------------------------------------------\n", "# Plot the resulting KDE by Transformation Method in [0,1]^2 \n", "#----------------------------------------------------------------------------------------------\n", "# define domain in [0,1]^2\n", "x_valuesU = np.arange(0,1,0.0025)\n", "y_valuesU = np.arange(0,1,0.0025)\n", "\n", "# transform to R^2 domain and make meshgrid\n", "x_hat_valuesR = norm.ppf(x_valuesU)\n", "y_hat_valuesR = norm.ppf(y_valuesU)\n", "x_hat_meshR, y_hat_meshR = np.meshgrid(x_hat_valuesR, y_hat_valuesR)\n", "\n", "# make predictions in R^2 domain and make meshgrid\n", "data_predictR = pd.DataFrame({\"Xhat\":x_hat_meshR.ravel(), \"Yhat\":y_hat_meshR.ravel()})\n", "z_hat_meshR = model.pdf(data_predict=data_predictR)\n", "\n", "# transform predictions in R^2 back to [0,1]^2 domain \n", "z_valuesU = np.array(z_hat_meshR / (data_predictR.Xhat.apply(norm.pdf)*data_predictR.Yhat.apply(norm.pdf)))\n", "\n", "# make meshgrid in [0,1]^2 domain for plotting\n", "x_meshU, y_meshU = np.meshgrid(x_valuesU, y_valuesU)\n", "z_meshU = z_valuesU.reshape(x_hat_meshR.shape)\n", "\n", "# collect sample data from KDE\n", "samples = sample_from_bivariate(x_meshU, y_meshU, z_meshU, n_samples)\n", "\n", "# 3d plot of resulting kde estimate of copula pdf in [0,1]^2\n", "from mpl_toolkits import mplot3d\n", "\n", "fig = plt.figure(figsize=(10,10))\n", "ax = fig.gca(projection='3d')\n", "surf = ax.plot_surface(x_meshU, y_meshU, z_meshU,\n", "                       linewidth=0, antialiased=False, cmap='viridis')\n", "ax.view_init(20, 270)\n", "\n", "# contour plot of resulting kde estimate of copula pdf in [0,1]^2\n", "plt.figure(figsize=(10,8))\n", "plt.contourf(x_meshU, y_meshU, z_meshU, 200, cmap='viridis')\n", "plt.xlim([0,1])\n", "plt.ylim([0,1])\n", "plt.colorbar()\n", "plt.title(\"Contour plot of copula pdf using Kernel Density Estimation via Transformation Method\")\n", "\n", "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,8))\n", "ax1.scatter(samples[:,0], samples[:,1],s=0.3)\n", "ax2.scatter(data[:,0], data[:,1],s=0.3)\n", "ax1.set_title(\"KDE Sample scatter plot\")\n", "ax2.set_title(\"Empirical data scatter plot\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Computationally, it seems we want to avoid using the KDE model itself to compute or sample from as it is computationally expensive. \n", "\n", "Taking note of [kdecopula: An R Package for the Kernel Estimation\n", "of Bivariate Copula Densities](https://arxiv.org/pdf/1603.04229.pdf), we will fit our KDE model to the transformed data and use spline interpolation to estimate the copula density. We will then use this spline estimate to carry out further operations and avoid using the KDE model. \n"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["from time import time\n", "from scipy.stats import norm\n", "import statsmodels.api as sm\n", "\n", "#-------------------------------------------------------------------------------------------\n", "# Fit Kernel Density Estimate to the data\n", "#-------------------------------------------------------------------------------------------\n", "# transform samples to R^2 domain\n", "data = np.array(marginal_values[list(pair)].values)\n", "X_ = norm.ppf(data)\n", "X_ = np_remove_inf(X_)\n", "\n", "# fit a kernel density estimate \n", "_BANDWIDTH = 'cv_ls' #'normal_reference' # 'cv_ml', 'cv_ls'\n", "kernel = sm.nonparametric.KDEMultivariate\n", "\n", "# fit the model to the transformed data\n", "t = time()\n", "model = kernel(data=X_, var_type='cc', bw=_BANDWIDTH)\n", "elapsed = time() - t \n", "\n", "print(\"Model fit in {} seconds\".format(elapsed))\n", "print(\"Model bandwidth {}\".format(model.bw))\n", "\n", "#-------------------------------------------------------------------------------------------\n", "# Generate pdf grid from model\n", "#-------------------------------------------------------------------------------------------\n", "\n", "# define domain in [0,1]^2\n", "x_valuesU = np.arange(0,1,0.0025)\n", "y_valuesU = np.arange(0,1,0.0025)\n", "\n", "# transform to R^2 domain and make meshgrid\n", "x_hat_valuesR_raw, y_hat_valuesR_raw = norm.ppf(x_valuesU), norm.ppf(y_valuesU)\n", "x_hat_valuesR, y_hat_valuesR = np_remove_inf_1D(x_hat_valuesR_raw), np_remove_inf_1D(x_hat_valuesR_raw)\n", "x_hat_meshR, y_hat_meshR = np.meshgrid(x_hat_valuesR, y_hat_valuesR)\n", "\n", "# make predictions in R^2 domain and make meshgrid\n", "data_predictR = pd.DataFrame({\"Xhat\":x_hat_meshR.ravel(), \"Yhat\":y_hat_meshR.ravel()})\n", "z_hat_meshR = model.pdf(data_predict=data_predictR)\n", "\n", "# transform predictions in R^2 back to [0,1]^2 domain \n", "z_valuesU = np.array(z_hat_meshR / (data_predictR.Xhat.apply(norm.pdf)*data_predictR.Yhat.apply(norm.pdf)))\n", "\n", "# make meshgrid in [0,1]^2 domain for plotting\n", "x_meshU, y_meshU = np.meshgrid(x_valuesU, y_valuesU)\n", "z_meshU = z_values.reshape(x_hat_meshR.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data.shape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#-----------------------------------------------------------------------------------------------\n", "# Use interpolation on pdf estimate \n", "# 1. Fit the interpolation on the R^2 domain as the interpolation itself is in a real domain\n", "# 2. Fit the interpolation on the [0,1]^2 domain and use bounding boxes to restrict the domain\n", "#-----------------------------------------------------------------------------------------------\n", "import scipy\n", "\n", "# Real domain\n", "interp_model = scipy.interpolate.RectBivariateSpline(x_hat_valuesR, y_hat_valuesR, z_meshU)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Z = interp_model(x_hat_valuesR, y_hat_valuesR)\n", "\n", "fig, ax = plt.subplots(nrows=1, ncols=2, subplot_kw={'projection': '3d'})\n", "ax[0].plot_wireframe(x_hat_valuesR, y_hat_valuesR, z_meshU, color='k')\n", "ax[1].plot_wireframe(x_hat_valuesR, y_hat_valuesR, Z, color='k')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_hat_valuesR"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 2}