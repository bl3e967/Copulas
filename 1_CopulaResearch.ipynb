{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n", "<hr>"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# utils \n", "from scipy.stats import iqr\n", "def freedman_diaconis(emp_data):\n", "    '''Return optimum bin width for empirical data for a histogram'''\n", "    IQR = iqr(emp_data)\n", "    n = len(emp_data)\n", "    return 2*IQR*np.power(n,-1/3)\n", "\n", "def square_root_choice(emp_data):\n", "    return np.ceil(np.sqrt(len(emp_data)))\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from statsmodels.distributions.empirical_distribution import ECDF\n", "from copulas.bivariate.base import Bivariate\n", "from copulas.multivariate import GaussianMultivariate\n", "\n", "class Parameter():\n", "    def __init__(self,\n", "                resolution,\n", "                start_year, \n", "                start_month=1, \n", "                start_day=1,\n", "                n_years=0,\n", "                n_months=0, \n", "                n_days=0): \n", "        \n", "        self.Resolution = resolution\n", "        \n", "        end_year = start_year + n_years\n", "        end_month = start_month + n_months\n", "        end_day = start_day + n_days \n", "        self.Start = datetime(start_year,start_month,start_day,9,30,0)\n", "        self.End = datetime(end_year, end_month, end_day,16,30,0)\n", "    \n", "class Data():\n", "    def __init__(self, df):\n", "        self._df = df.dropna()\n", "    \n", "    @property \n", "    def prices(self):\n", "        return self._df\n", "    \n", "    @property\n", "    def returns(self):\n", "        return self._df.diff() \n", "    \n", "    @property\n", "    def columns(self):\n", "        return self._df.columns\n", "    \n", "    def plot(self, *args, **kwargs): \n", "        return self._df.plot(*args, **kwargs)\n", "    \n", "class EmpiricalCopula():\n", "    def __init__(self, X):\n", "        '''\n", "            args:\n", "                X: numpy array of shape (n_pairs, 2)\n", "        '''\n", "        self.data = X\n", "        self._len = X.shape[0]\n", "    \n", "    def __len__(self):\n", "        return self._len\n", "    \n", "    @staticmethod \n", "    def _get_bin_width(self, data):\n", "        return freedman_diaconis(data)\n", "    \n", "    def cdf(self, u, v):\n", "        '''returns C(u,v) = P(u <= U, v <= V)'''\n", "        cnt = np.sum((self.data[:,0] <= u) & (self.data[:,1] <= v))\n", "        return float(cnt / self._len)\n", "    \n", "    def pdf(self, u, v):\n", "        pass \n", "    \n", "    def cond_pdf(u, given_p):\n", "        '''P(U<=u | V=v) = dC(u,v)/dv = P(U<=u, V=v)/p(V=v)'''\n", "        pass \n", "        \n", "def emp_copula_test():\n", "    u = np.expand_dims(np.arange(0,100,1)+1, axis=1)\n", "    v = np.expand_dims(np.arange(0,100,1)+1, axis=1)\n", "    x = np.concatenate((u,v),axis=1)\n", "    print(\"u:\",u.shape)\n", "    print(\"v:\",v.shape)\n", "    print(\"x:\",x.shape)\n", "        \n", "    test_copula = EmpiricalCopula(x)\n", "    assert len(test_copula) == len(u), f\"{len(test_copula)} != {len(u)}\"\n", "    \n", "    pvalue = test_copula.cdf(50,50)\n", "    assert pvalue == 0.5, f\"p = {pvalue} not 0.5\"\n", "    pvalue = test_copula.cdf(25,25)\n", "    assert pvalue == 0.25, f\"p = {pvalue} not 0.25\"\n", "    pvalue = test_copula.cdf(100,0)\n", "    assert pvalue == 0.0, f\"p={pvalue} not 0\"\n", "    pvalue = test_copula.cdf(0,100)\n", "    assert pvalue == 0.0, f\"p={pvalue} not 0\"\n", "    pvalue = test_copula.cdf(20,100)\n", "    assert pvalue == 0.2, f\"p={pvalue} not 0.2\"\n", "    pvalue = test_copula.cdf(100,20)\n", "    assert pvalue == 0.2, f\"p={pvalue} not 0.2\"\n", "    pvalue = test_copula.cdf(100,100)\n", "    assert pvalue == 1.0, f\"p={pvalue} not 1.0\"\n", "    \n", "emp_copula_test()\n", "                \n", "        \n", "            \n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# QuantBook Analysis Tool \n", "# For more information see [https://www.quantconnect.com/docs/research/overview]\n", "qb = QuantBook()\n", "\n", "# parameters\n", "START_YEAR = 2005\n", "START_MONTH = 1\n", "START_DAY = 1\n", "N_YEARS = 10\n", "N_MONTHS = 0\n", "N_DAYS = 0\n", "RESOLUTION = Resolution.Daily\n", "PARAM = Parameter(RESOLUTION, START_YEAR, START_MONTH, START_DAY,\n", "                  N_YEARS, N_MONTHS, N_DAYS)\n", "\n", "# Specify list of correlated tickers for S&P 500 \n", "tickers = [\"SPY\",\"XLK\", \"VGT\", \"IYW\", \"IGV\"]\n", "\n", "# register tickers to quantbook\n", "for ticker in tickers: \n", "    qb.AddEquity(ticker)\n", "    \n", "# get historical prices\n", "history = qb.History(tickers, PARAM.Start, PARAM.End, PARAM.Resolution)\n", "\n", "# Unpack dataframe\n", "Open = history['open'].unstack(level=0)\n", "Close = history['close'].unstack(level=0)\n", "High = history['high'].unstack(level=0)\n", "Low = history['low'].unstack(level=0)\n", "\n", "# create data object\n", "close = Data(Close)\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# Calculate Kendall Tau\n", "corr = close.returns.corr(method='kendall')\n", "corr.style.background_gradient(cmap='viridis')"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# Empirically estimate marginal distributions\n", "marginal_dist = {}\n", "for asset in close.columns:\n", "    marginal_dist[asset] = ECDF(close.returns[asset])\n", "\n", "marginal_values = pd.DataFrame()\n", "for asset in close.columns: \n", "    marginal_values[asset] = close.returns[asset].apply(marginal_dist[asset])"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["def get_max_corr_pair(corr_matrix): \n", "    sol = (corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n", "                 .stack()\n", "                 .sort_values(ascending=False))\n", "    \n", "    col1 = close.columns[sol.index[sol == max(sol)].labels[0]].values[0]\n", "    col2 = close.columns[sol.index[sol == max(sol)].labels[1]].values[0]\n", "    return(col1, col2)\n", "\n", "pair = get_max_corr_pair(corr)\n", "print(\"Pair with maximum correlation: {}\".format(pair))\n", "\n", "X = np.array(marginal_values[list(pair)].values)"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["close_zeroed = (close._df - close._df.iloc[0])\n", "close_norm = (close_zeroed - close_zeroed.mean())/close_zeroed.std()\n", "close_norm[[close_norm.columns[3], close_norm.columns[2]]].plot(figsize=(20,10))\n", "\n", "diff = close_norm[close_norm.columns[3]] - close_norm[close_norm.columns[2]]\n", "plt.figure()\n", "diff.plot(figsize=(20,10))\n", "\n", "plt.figure(figsize=(20,10))\n", "plt.scatter(close_norm[close_norm.columns[3]], close_norm[close_norm.columns[2]])"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": ["from statsmodels.tsa.stattools import adfuller\n", "result = adfuller(diff)\n", "print('ADF Statistic: %f' % result[0])\n", "print('p-value: %f' % result[1])\n", "print('Critical Values:')\n", "for key, value in result[4].items():\n", "\tprint('\\t%s: %.3f' % (key, value))"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"scrolled": false}, "outputs": [], "source": ["# data \n", "plt.figure(figsize=(10,10))\n", "plt.scatter(X[:,0], X[:,1], s=1.0)\n", "plt.grid()\n", "\n", "# gumbel sample data \n", "gumbel = Bivariate(copula_type=\"clayton\")\n", "gumbel.fit(X)\n", "samples = gumbel.sample(len(marginal_values))\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(samples[:,0], samples[:,1], s=1.0)\n", "\n", "# gauss sample data\n", "gauss = GaussianMultivariate()\n", "gauss.fit(X)\n", "samples = gauss.sample(len(marginal_values))\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(samples[0], samples[1], s=1.0)\n", "\n", "plt.grid()\n", "\n", "from matplotlib.image import NonUniformImage\n", "fig = plt.figure(figsize=(10, 10))\n", "H, xedges, yedges = np.histogram2d(X[:,0], X[:,1], bins=50, density=True)\n", "# x, y = np.meshgrid(xedges, yedges)\n", "ax = fig.add_subplot(title='NonUniformImage: interpolated',\n", "                     aspect='equal', xlim=xedges[[0, -1]], ylim=yedges[[0, -1]])\n", "im = NonUniformImage(ax, interpolation='bilinear')\n", "xcenters = (xedges[:-1] + xedges[1:]) / 2\n", "ycenters = (yedges[:-1] + yedges[1:]) / 2\n", "\n", "# print(ycenters)\n", "im.set_data(xcenters, ycenters, H)\n", "im.set_extent(im.get_extent()) # workaround for valuetype error\n", "# print(im._extent)\n", "ax.images.append(im)\n", "\n", "print(H)\n", "# plt.show()\n", "\n", "# plt.figure(figsize=(20,20))\n", "# ax = fig.add_subplot(title='pcolormesh: actual edges',\n", "#          aspect='equal')\n", "# x, y = np.meshgrid(xedges, yedges)\n", "# ax.pcolormesh(x, y, H)\n", "# plt.show()\n", "# print(histgrid)\n", "# print(len(X))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use Kernel Density Estimation to estimate the copula density\n", "\n", "### 1. Naive Kernel Density Estimation\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["\n", "from scipy import stats\n", "from sklearn.neighbors import KernelDensity \n", "from sklearn.model_selection import GridSearchCV\n", "import multiprocessing as mp\n", "\n", "_USE_CV = True\n", "\n", "if _USE_CV: \n", "    # use grid search cross-validation to optimize the bandwidth for KDE\n", "    params = {'bandwidth': np.logspace(-5, 0, 20)}\n", "    _NJOBS = 10\n", "    _CV_SIZE = len(X) # use leave-one-out cv as sample size is small \n", "    grid = GridSearchCV(KernelDensity(breadth_first=False), params, verbose=1, n_jobs=_NJOBS, cv=_CV_SIZE)\n", "    grid.fit(X)\n", "\n", "\n", "    print(\"Best Esimator Bandwidth: {}\".format(grid.best_estimator_.bandwidth))\n", "    model = grid.best_estimator_\n", "    \n", "else: \n", "    _KERNEL = 'gaussian'\n", "    _BANDWIDTH = float(1/10)\n", "    model = KernelDensity(kernel=_KERNEL, bandwidth=_BANDWIDTH)\n", "    model.fit(X)\n", "\n", "samples = model.sample(len(X))"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["get_n_bins = lambda x: square_root_choice(x) # lambda x: (max(x)-min(x))/ freedman_diaconis(x)\n", "\n", "u_nbins = get_n_bins(X[:,0])\n", "v_nbins = get_n_bins(X[:,1])\n", "\n", "u_edges = np.linspace(min(X[:,0]), max(X[:,0]), u_nbins)\n", "v_edges = np.linspace(min(X[:,1]), max(X[:,1]), v_nbins)\n", "\n", "H, xedges, yedges = np.histogram2d(X[:,0], X[:,1], bins=[u_nbins, v_nbins], normed=True)\n", "\n", "xcenters = (xedges[:-1] + xedges[1:]) / 2\n", "ycenters = (yedges[:-1] + yedges[1:]) / 2\n", "\n", "\n", "# original \n", "fig = plt.figure(figsize=(10, 10))\n", "ax = fig.add_subplot(title='NonUniformImage: interpolated',\n", "                     aspect='equal', xlim=xedges[[0, -1]], ylim=yedges[[0, -1]])\n", "im = NonUniformImage(ax, interpolation='bilinear')\n", "im.set_data(xcenters, ycenters, H)\n", "im.set_extent(im.get_extent()) # workaround for valuetype error\n", "ax.images.append(im)\n", "\n", "# samples\n", "H, xedges, yedges = np.histogram2d(samples[:,0], samples[:,1], bins=[u_nbins, v_nbins], normed=True)\n", "fig = plt.figure(figsize=(10, 10))\n", "ax = fig.add_subplot(title='NonUniformImage: interpolated',\n", "                     aspect='equal', xlim=xedges[[0, -1]], ylim=yedges[[0, -1]])\n", "im = NonUniformImage(ax, interpolation='bilinear')\n", "im.set_data(xcenters, ycenters, H)\n", "im.set_extent(im.get_extent()) # workaround for valuetype error\n", "ax.images.append(im)"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["from mpl_toolkits.mplot3d import Axes3D\n", "from scipy import interpolate\n", "\n", "H, xedges, yedges = np.histogram2d(X[:,0], X[:,1], bins=[u_nbins, v_nbins], normed=True)\n", "\n", "xpos, ypos = np.meshgrid(xedges[:-1] + 0.001, yedges[:-1] + 0.001, indexing=\"ij\")\n", "xpos = xpos.ravel()\n", "ypos = ypos.ravel()\n", "zpos = 0\n", "\n", "\n", "dx = (1/u_nbins)*np.ones_like(zpos)\n", "dy =  (1/v_nbins)* np.ones_like(zpos)\n", "dz = H.ravel()\n", "tck = interpolate.bisplrep(xpos, ypos, dz)\n", "xnew, ynew = np.mgrid[0:1:100j, 0:1:100j]\n", "z = interpolate.bisplev(xnew[:,0], ynew[0,:], tck)\n", "plt.figure(figsize=(15,10))\n", "plt.pcolor(xnew, ynew, z)\n", "plt.colorbar()\n", "\n", "\n", "fig = plt.figure(figsize=(20,20))\n", "ax = fig.add_subplot(111, projection='3d')\n", "ax.bar3d(xpos, ypos, zpos, dx, dy, dz, zsort='average')\n", "ax.plot_surface(xnew, ynew, z,cmap='viridis', edgecolor='none')\n", "ax.view_init(45, 40)\n"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["print(np.min(z))"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(2, figsize=(10,20))\n", "ax[0].scatter(X[:,0],X[:,1], s=0.3)\n", "ax[1].scatter(samples[:,0],samples[:,1], s=0.3)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Transformation Estimator\n", "\n", "Let $\\Phi$ be the standard normal cdf and $\\phi$ its density. Then the random vector $(X,Y) = (\\phi^{-1}(U), \\phi^{-1}(V))$ has normally distributed margins and is supported on the full $\\mathbb{R}^2$. By Sklar's Theroem, its density $f$ can be written as: \n", "\n", "$$f(x,y) = c(\\Phi(x), \\Phi(y))\\phi(x)\\phi(y)$$\n", "\n", "To estimate this density, we transform samples $(U_i,V_i)$ to $(X_i,Y_i) = (\\Phi^{-1}(U_i), \\Phi^{-1}(V_i))$ for $i = 1,...,n$. To this transformed sample, we now apply the standard kernel density estimator for $(x,y) \\in \\mathbb{R}^2$: \n", "\n", "$$\\hat{f}_n(x,y) = \\frac{1}{n}\\sum_{i=1}^{n}K_{b_n}(x-X_i)K_{b_n}(y-Y_i)$$\n", "\n", "Hence, the copula density can be expressed as : \n", "\n", "$$\n", "    \\hat{c}_n(u,v) = \\frac{\\sum_{i=1}^{n}K_{b_n}\\big(\\Phi^{-1}(u) - \\Phi^{-1}(U_i)\\big)K_{b_n}\\big(\\Phi^{-1}(v) - \\Phi^{-1}(V_i)\\big)}{n\\phi(\\Phi^{-1}(u))\\phi(\\Phi^{-1}(v))}\n", "$$\n", "for all $(u,v) \\in [0,1]^2$"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["from scipy.stats import norm\n", "\n", "inv_Phi = norm.ppf\n", "Phi = norm.cdf\n", "\n", "# transform from [0,1]^2 to R^2 domain\n", "W = inv_Phi(X)\n", "\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(W[:,0], W[:,1], s=0.3)\n", "\n", "# use grid search cross-validation to optimize the bandwidth for KDE\n", "params = {'bandwidth': np.logspace(-5, 0, 20)}\n", "_NJOBS = 10\n", "_CV_SIZE = len(W)-1 # use leave-one-out cv as sample size is small \n", "grid = GridSearchCV(KernelDensity(breadth_first=False), params, verbose=1, n_jobs=_NJOBS, cv=_CV_SIZE)\n", "grid.fit(W[1:,:])\n", "\n", "\n", "print(\"Best Esimator Bandwidth: {}\".format(grid.best_estimator_.bandwidth))\n", "model = grid.best_estimator_\n", "\n", "samples = model.sample(len(W)*500)\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(samples[:,0], samples[:,1], s=0.3)\n", "\n"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["x_axis, y_axis = np.meshgrid(samples[:,0], samples[:,1])\n", "f = model.score_samples()\n", "print(x_axis)"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["mean = np.mean(W[1:,:],axis=0)\n", "cov = np.cov(W[1:,:], rowvar=0)\n", "print(mean)\n", "print(cov)\n", "from scipy.stats import multivariate_normal \n", "rv = multivariate_normal(mean, cov)\n", "samples = rv.rvs(len(W))\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(samples[:,0], samples[:,1], s=0.3)\n", "\n", "\n", "# convert samples back to [0,1]^2 domain\n", "invg_u = norm.pdf(samples[:,0])\n", "invg_v = norm.pdf(samples[:,1])\n", "\n", "den = np.multiply(invg_u, invg_v)\n"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["den"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["close.columns\n", "asset1, asset2 = \"VGT 2T\", \"XLK 2T\"\n", "plt.figure(figsize=(10,10))\n", "plt.scatter(close.returns[asset1].values, close.returns[asset2].values, s=3)\n", "\n", "from sklearn import linear_model\n", "ransac = linear_model.RANSACRegressor()\n", "ransac.fit(close.returns[asset1].dropna().values.reshape(-1,1),close.returns[asset2].dropna().values.reshape(-1,1))\n", "ransac_x = np.arange(close.returns[asset1].dropna().values.min(), close.returns[asset1].dropna().values.max(), 0.001)\n", "ransac_y = ransac.predict(ransac_x.reshape(-1,1))\n", "\n", "\n", "ridge = linear_model.RANSACRegressor()\n", "ridge.fit(close.returns[asset1].dropna().values.reshape(-1,1),close.returns[asset2].dropna().values.reshape(-1,1))\n", "ridge_x = np.arange(close.returns[asset1].dropna().values.min(), close.returns[asset1].dropna().values.max(), 0.001)\n", "ridge_y = ridge.predict(ridge_x.reshape(-1,1))\n", "\n", "plt.scatter(ransac_x, ransac_y, color='r', s=0.5)\n", "plt.scatter(ridge_x, ridge_y, color='g', s=0.5)\n", "plt.grid()"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["close.returns['IGV 2T'].dropna().values.shape"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["ransac_x = np.arange(close.returns[asset1].dropna().values.min(), close.returns[asset1].dropna().values.max())"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["\n", "coeff_list = []\n", "for i in range(1000):\n", "    ransac = linear_model.RANSACRegressor()\n", "    ransac.fit(close.returns[asset1].dropna().values.reshape(-1,1),close.returns[asset2].dropna().values.reshape(-1,1))\n", "    coeff_list.append(ransac.estimator_.coef_)"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["array = np.array(coeff_list).flatten()\n", "plt.hist(array, bins=250)\n", "from scipy.stats import mode \n", "binned = np.histogram(array, bins=250)\n", "7\n", "coeff = binned[1][np.argmax(binned[0])]\n", "\n", "\n", "plt.figure(figsize=(20,10))\n", "plt.scatter(close.returns[asset1].values, close.returns[asset2].values, s=3)\n", "\n", "x = np.arange(close.returns[asset1].dropna().values.min(), close.returns[asset1].dropna().values.max(), 0.001)\n", "y = coeff*x\n", "plt.scatter(x,y,s=3)\n", "# plt.scatter(ridge_x, ridge_y, color='g', s=0.5)\n"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(20,10))\n", "(close.prices[asset2]-close.prices[asset2].iloc[0]).plot(linewidth=0.3)\n", "((close.prices[asset1]-close.prices[asset1].iloc[0])*coeff).plot(linewidth=0.3)"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 2}